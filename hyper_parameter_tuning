{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":12041030,"sourceType":"datasetVersion","datasetId":7576939}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install rdkit-pypi rdkit torch transformers pytorch-lightning requests matplotlib -q","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport sys\nimport torch\nimport logging\nimport pandas as pd\nimport numpy as np\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom rdkit import Chem, RDLogger\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom scipy.stats import pearsonr\nimport pytorch_lightning as pl\nfrom pytorch_lightning.callbacks import EarlyStopping\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\", handlers=[logging.StreamHandler(sys.stdout)], force=True)\nlogger = logging.getLogger(__name__)\n\nRDLogger.DisableLog('rdApp.*')\n\ndef canonicalize_smiles(smiles):\n    if not isinstance(smiles, str):\n        return None\n    try:\n        mol = Chem.MolFromSmiles(smiles)\n        if mol is None:\n            return None\n        return Chem.MolToSmiles(mol, canonical=True)\n    except Exception:\n        return None\n\ndef load_aid_dataset(path, protein_sequence, max_rows=None):\n    df = pd.read_csv(path, sep=',', dtype=str, on_bad_lines='skip')\n    df.columns = [c.strip() for c in df.columns]\n    required_cols = ['PUBCHEM_EXT_DATASOURCE_SMILES', 'Fit_LogAC50']\n    if not all(c in df.columns for c in required_cols):\n        logger.error(f\"Missing required columns in {path}. Available: {df.columns}\")\n        return pd.DataFrame(columns=['SMILES', 'Target Sequence', 'log_kd'])\n    df = df[required_cols].copy()\n    df = df.rename(columns={'PUBCHEM_EXT_DATASOURCE_SMILES': 'SMILES', 'Fit_LogAC50': 'log_kd'})\n    # Filter non-string SMILES before canonicalization\n    before = len(df)\n    df = df[df['SMILES'].apply(lambda x: isinstance(x, str))]\n    logger.info(f\"AID: Filtered non-string SMILES: {before - len(df)} removed, {len(df)} remain.\")\n    df['SMILES'] = df['SMILES'].apply(canonicalize_smiles)\n    after = len(df)\n    df = df.dropna(subset=['SMILES', 'log_kd'])\n    logger.info(f\"AID: After canonicalization and dropna: {after - len(df)} removed, {len(df)} remain.\")\n    df['log_kd'] = pd.to_numeric(df['log_kd'], errors='coerce')\n    df = df.dropna(subset=['log_kd'])\n    df['Target Sequence'] = protein_sequence\n    if max_rows:\n        df = df.head(max_rows)\n    logger.info(f\"AID: Final loaded rows: {len(df)}\")\n    return df[['SMILES', 'Target Sequence', 'log_kd']]\n\ndef stratified_sample(df, n_samples, n_bins=20, random_state=42):\n    if df.empty:\n        return df\n    bins = n_bins\n    while bins > 1:\n        try:\n            df = df.copy()\n            df['log_kd_bin'] = pd.qcut(df['log_kd'], q=bins, duplicates='drop')\n            bin_counts = df['log_kd_bin'].value_counts()\n            valid_bins = bin_counts[bin_counts > 0].index\n            df = df[df['log_kd_bin'].isin(valid_bins)]\n            sampled = (\n                df.groupby('log_kd_bin', group_keys=False, observed=True)\n                .apply(lambda x: x.sample(min(len(x), max(1, int(n_samples / bins))), random_state=random_state))\n            )\n            sampled = sampled.sample(frac=1, random_state=random_state).drop(columns=['log_kd_bin'])\n            if not sampled.empty:\n                return sampled.head(n_samples)\n        except Exception:\n            pass\n        bins -= 1\n    return df.sample(n=min(n_samples, len(df)), random_state=random_state)\n\nclass SimpleDTIDataset(Dataset):\n    def __init__(self, df, tokenizer, log_kd_mean, log_kd_std):\n        self.df = df.reset_index(drop=True)\n        self.tokenizer = tokenizer\n        self.log_kd_mean = log_kd_mean\n        self.log_kd_std = log_kd_std\n    def __len__(self):\n        return len(self.df)\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        tokens = self.tokenizer.tokenize(row['SMILES'], row['Target Sequence'])\n        if tokens is None:\n            return None\n        norm_log_kd = (row['log_kd'] - self.log_kd_mean) / self.log_kd_std\n        return {\n            **tokens,\n            'log_kd': torch.tensor(norm_log_kd, dtype=torch.float32),\n            'original_log_kd': torch.tensor(row['log_kd'], dtype=torch.float32),\n            'smiles': row['SMILES'],\n            'protein': row['Target Sequence']\n        }\n\ndef collate_fn(batch):\n    batch = [b for b in batch if b is not None]\n    if not batch:\n        return None\n    return {\n        \"smiles_input_ids\": torch.stack([b[\"smiles_input_ids\"] for b in batch]),\n        \"smiles_attention_mask\": torch.stack([b[\"smiles_attention_mask\"] for b in batch]),\n        \"protein_input_ids\": torch.stack([b[\"protein_input_ids\"] for b in batch]),\n        \"protein_attention_mask\": torch.stack([b[\"protein_attention_mask\"] for b in batch]),\n        \"log_kd\": torch.stack([b[\"log_kd\"] for b in batch]),\n        \"original_log_kd\": torch.stack([b[\"original_log_kd\"] for b in batch]),\n        \"smiles\": [b[\"smiles\"] for b in batch],\n        \"protein\": [b[\"protein\"] for b in batch]\n    }\n\nclass SMILESProteinTokenizer:\n    def __init__(self, max_len_smiles=128, max_len_protein=512):\n        self.max_len_smiles = max_len_smiles\n        self.max_len_protein = max_len_protein\n        self.smiles_tokenizer = AutoTokenizer.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n        self.protein_tokenizer = AutoTokenizer.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n    def tokenize(self, smiles, protein):\n        try:\n            smiles_enc = self.smiles_tokenizer(smiles, max_length=self.max_len_smiles, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n            protein_enc = self.protein_tokenizer(protein, max_length=self.max_len_protein, padding=\"max_length\", truncation=True, return_tensors=\"pt\")\n            return {\n                \"smiles_input_ids\": smiles_enc[\"input_ids\"].squeeze(),\n                \"smiles_attention_mask\": smiles_enc[\"attention_mask\"].squeeze(),\n                \"protein_input_ids\": protein_enc[\"input_ids\"].squeeze(),\n                \"protein_attention_mask\": protein_enc[\"attention_mask\"].squeeze()\n            }\n        except Exception:\n            return None\n\nclass DTIModel(pl.LightningModule):\n    def __init__(self, hidden_dim=256, learning_rate=1e-4, log_kd_mean=0.0, log_kd_std=1.0):\n        super().__init__()\n        self.save_hyperparameters()\n        self.smiles_encoder = AutoModel.from_pretrained(\"seyonec/PubChem10M_SMILES_BPE_450k\")\n        self.protein_encoder = AutoModel.from_pretrained(\"facebook/esm2_t12_35M_UR50D\")\n        self.smiles_dim = self.smiles_encoder.config.hidden_size\n        self.protein_dim = self.protein_encoder.config.hidden_size\n        self.protein_preproj = torch.nn.Linear(self.protein_dim, self.smiles_dim)\n        self.smiles_proj = torch.nn.Linear(self.smiles_dim, hidden_dim)\n        self.protein_proj = torch.nn.Linear(self.smiles_dim, hidden_dim)\n        self.regression_head = torch.nn.Sequential(\n            torch.nn.Linear(hidden_dim * 2, hidden_dim),\n            torch.nn.ReLU(),\n            torch.nn.Dropout(0.2),\n            torch.nn.Linear(hidden_dim, 1)\n        )\n        self.criterion = torch.nn.MSELoss()\n    def forward(self, smiles_input_ids, smiles_attention_mask, protein_input_ids, protein_attention_mask):\n        smiles_embeds = self.smiles_encoder(input_ids=smiles_input_ids, attention_mask=smiles_attention_mask).last_hidden_state\n        protein_embeds_orig = self.protein_encoder(input_ids=protein_input_ids, attention_mask=protein_attention_mask).last_hidden_state\n        protein_embeds_proj = self.protein_preproj(protein_embeds_orig)\n        smiles_pooled = smiles_embeds.mean(dim=1)\n        protein_pooled = protein_embeds_proj.mean(dim=1)\n        smiles_final_proj = self.smiles_proj(smiles_pooled)\n        protein_final_proj = self.protein_proj(protein_pooled)\n        combined_features = torch.cat([smiles_final_proj, protein_final_proj], dim=-1)\n        log_kd_pred_normalized = self.regression_head(combined_features).squeeze(-1)\n        return log_kd_pred_normalized\n    def training_step(self, batch, batch_idx):\n        log_kd_pred_normalized = self(batch[\"smiles_input_ids\"], batch[\"smiles_attention_mask\"], batch[\"protein_input_ids\"], batch[\"protein_attention_mask\"])\n        targets_normalized = batch[\"log_kd\"]\n        loss = self.criterion(log_kd_pred_normalized, targets_normalized)\n        self.log(\"train_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        return loss\n    def validation_step(self, batch, batch_idx):\n        log_kd_pred_normalized = self(batch[\"smiles_input_ids\"], batch[\"smiles_attention_mask\"], batch[\"protein_input_ids\"], batch[\"protein_attention_mask\"])\n        targets_normalized = batch[\"log_kd\"]\n        loss = self.criterion(log_kd_pred_normalized, targets_normalized)\n        self.log(\"val_loss\", loss, on_step=False, on_epoch=True, prog_bar=True)\n        with torch.no_grad():\n            denormalized_pred = log_kd_pred_normalized * self.hparams.log_kd_std + self.hparams.log_kd_mean\n            original_targets = batch[\"original_log_kd\"]\n        return {\"loss\": loss.item(), \"preds\": denormalized_pred.cpu().numpy(), \"targets\": original_targets.cpu().numpy()}\n    def configure_optimizers(self):\n        return torch.optim.AdamW(self.parameters(), lr=self.hparams.learning_rate)\n\ndef main():\n    # Paths (update as needed)\n    aid_504832_path = \"/kaggle/input/punched-aid/AID_504832.csv\"\n    aid_504834_path = \"/kaggle/input/punched-aid/AID_504834.csv\"\n    malaria_protein_seq = \"MMLYISAKKAQVAFILYIVLVLRIISGNNDFCKPSSLNSEISGFIGYKCNFSNEGVHNLKPDMRERRSIFCTIHSYFIYDKIRLIIPKKSSSPEFKILPEKCFQKVYTDYENRVETDISELGLIEYEIEENDTNPNYNERTITISPFSPKDIEFFCFCDNTEKVISSIEGRSAMVHVRVLKYPHNILFTNLTNDLFTYLPKTYNESNFVSNVLEVELNDGELFVLACELINKKCFQEGKEKALYKSNKIIYHKNLTIFKAPFYVTSKDVNTECTCKFKNNNYKIVLKPKYEKKVIHGCNFSSNVSSKHTFTDSLDISLVDDSAHISCNVHLSEPKYNHLVGLNCPGDIIPDCFFQVYQPESEELEPSNIVYLDSQINIGDIEYYEDAEGDDKIKLFGIVGSIPKTTSFTCICKKDKKSAYMTVTIDSAYYGFLAKTFIFLIVAILLYI\"\n    tokenizer = SMILESProteinTokenizer()\n    # Use only AID datasets for tuning\n    aid_504832_df = load_aid_dataset(aid_504832_path, malaria_protein_seq, max_rows=100000)\n    aid_504832_df = stratified_sample(aid_504832_df, 5000, n_bins=20)\n    logger.info(f\"Loaded {len(aid_504832_df)} stratified samples from AID 504832.\")\n    aid_504834_df = load_aid_dataset(aid_504834_path, malaria_protein_seq, max_rows=100000)\n    aid_504834_df = stratified_sample(aid_504834_df, 5000, n_bins=20)\n    logger.info(f\"Loaded {len(aid_504834_df)} stratified samples from AID 504834.\")\n    all_df = pd.concat([aid_504832_df, aid_504834_df], ignore_index=True)\n    all_df = all_df.drop_duplicates(subset=[\"SMILES\", \"Target Sequence\"]).reset_index(drop=True)\n    logger.info(f\"Total unique samples for tuning: {len(all_df)}\")\n    # Split for tuning\n    all_df['log_kd_bin'] = pd.qcut(all_df['log_kd'], q=10, duplicates='drop', labels=False)\n    train_df, val_df = train_test_split(all_df, test_size=0.2, random_state=42, stratify=all_df['log_kd_bin'])\n    log_kd_mean = train_df['log_kd'].mean()\n    log_kd_std = train_df['log_kd'].std() if train_df['log_kd'].std() > 0 else 1.0\n    train_loader = DataLoader(SimpleDTIDataset(train_df, tokenizer, log_kd_mean, log_kd_std), batch_size=32, shuffle=True, num_workers=2, collate_fn=collate_fn)\n    val_loader = DataLoader(SimpleDTIDataset(val_df, tokenizer, log_kd_mean, log_kd_std), batch_size=32, num_workers=2, collate_fn=collate_fn)\n    # Small hyperparameter grid\n    param_grid = [\n        {\"learning_rate\": 1e-4, \"hidden_dim\": 256},\n        {\"learning_rate\": 5e-5, \"hidden_dim\": 256},\n        {\"learning_rate\": 1e-4, \"hidden_dim\": 512},\n    ]\n    best_r2 = -float('inf')\n    best_params = None\n    for params in param_grid:\n        logger.info(f\"Tuning trial: {params}\")\n        model = DTIModel(hidden_dim=params[\"hidden_dim\"], learning_rate=params[\"learning_rate\"], log_kd_mean=log_kd_mean, log_kd_std=log_kd_std)\n        early_stop = EarlyStopping(monitor=\"val_loss\", patience=2, mode=\"min\", verbose=True)\n        trainer = pl.Trainer(max_epochs=8, callbacks=[early_stop], logger=False, enable_progress_bar=True, accelerator='gpu' if torch.cuda.is_available() else 'cpu', devices=1)\n        trainer.fit(model, train_loader, val_loader)\n        # Evaluate on val set\n        val_outputs = trainer.validate(model, val_loader, verbose=False)\n        preds, targets = [], []\n        for batch in val_loader:\n            if batch is None: continue\n            with torch.no_grad():\n                pred = model(batch[\"smiles_input_ids\"].to(model.device), batch[\"smiles_attention_mask\"].to(model.device), batch[\"protein_input_ids\"].to(model.device), batch[\"protein_attention_mask\"].to(model.device))\n                preds.extend((pred * log_kd_std + log_kd_mean).cpu().numpy())\n                targets.extend(batch[\"original_log_kd\"].cpu().numpy())\n        if len(targets) > 1 and np.var(targets) > 1e-6:\n            r2 = r2_score(targets, preds)\n            logger.info(f\"Trial {params}: R2={r2:.4f}\")\n            if r2 > best_r2:\n                best_r2 = r2\n                best_params = params\n    logger.info(f\"Best hyperparameters: {best_params} (R2={best_r2:.4f})\")\n    # Save best params\n    pd.Series(best_params).to_json(\"best_hyperparams.json\")\n\nif __name__ == \"__main__\":\n    main()","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}